#!/usr/bin/env python3
"""
èœè°±æ•°æ®å¯¼å…¥è„šæœ¬ï¼ˆå¹¶å‘ä¼˜åŒ–ç‰ˆï¼‰

ç”¨æ³•:
    python scripts/import_recipes.py --input recipes.json
    python scripts/import_recipes.py --input recipes.json --collection custom_collection
    python scripts/import_recipes.py --input recipes.json --batch-size 200 --workers 5
"""
import json
import argparse
import sys
import time
import threading
from pathlib import Path
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.documents import Document

# å†™å…¥é”ï¼Œç¡®ä¿ ChromaDB å†™å…¥æ“ä½œçš„çº¿ç¨‹å®‰å…¨
_write_lock = threading.Lock()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.core.rag.vectorstore import get_vectorstore
from app.core.config import settings


def load_json_data(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½ JSON/JSONL æ•°æ®æ–‡ä»¶
    æ”¯æŒæ ¼å¼ï¼š
        1. JSON æ•°ç»„: [{}, {}, ...]
        2. JSONL æ ¼å¼: {}\n{}\n... (æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡)
        3. å•ä¸ª JSON å¯¹è±¡: {}
    
    Args:
        file_path: JSON/JSONL æ–‡ä»¶è·¯å¾„
    
    Returns:
        List[Dict]: èœè°±æ•°æ®åˆ—è¡¨
    """
    recipes = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        # è¯»å–ç¬¬ä¸€è¡Œåˆ¤æ–­æ ¼å¼
        first_line = f.readline().strip()
        f.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ JSONL æ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰
        if first_line.startswith('{') and not first_line.startswith('[{'):
            # JSONL æ ¼å¼ï¼šé€è¡Œè¯»å–
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:  # è·³è¿‡ç©ºè¡Œ
                    continue
                try:
                    recipe = json.loads(line)
                    recipes.append(recipe)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  ç¬¬ {line_num} è¡Œ JSON è§£æå¤±è´¥ï¼ˆè·³è¿‡ï¼‰: {e}")
                    continue
        else:
            # æ ‡å‡† JSON æ ¼å¼ï¼ˆæ•°ç»„æˆ–å•ä¸ªå¯¹è±¡ï¼‰
            try:
                data = json.load(f)
                if isinstance(data, list):
                    recipes = data
                elif isinstance(data, dict):
                    recipes = [data]
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„ JSON æ ¼å¼: {type(data)}")
            except json.JSONDecodeError as e:
                raise ValueError(f"JSON è§£æå¤±è´¥: {e}")
    
    return recipes


def recipe_to_document(recipe: Dict[str, Any]) -> Document:
    """
    å°†èœè°±æ•°æ®è½¬æ¢ä¸º LangChain Document
    
    Args:
        recipe: èœè°±å­—å…¸
    
    Returns:
        Document: LangChain æ–‡æ¡£å¯¹è±¡
    """
    # æ„å»ºæ–‡æ¡£å†…å®¹
    content_parts = []
    
    # èœå
    if recipe.get('name'):
        content_parts.append(f"èœåï¼š{recipe['name']}")
    
    # æ ‡å‡†èœå
    if recipe.get('dish') and recipe['dish'] != 'Unknown':
        content_parts.append(f"æ ‡å‡†èœåï¼š{recipe['dish']}")
    
    # æè¿°
    if recipe.get('description'):
        content_parts.append(f"æè¿°ï¼š{recipe['description']}")
    
    # é£Ÿæ
    if recipe.get('recipeIngredient'):
        ingredients = ', '.join(recipe['recipeIngredient'])
        content_parts.append(f"é£Ÿæï¼š{ingredients}")
    
    # æ­¥éª¤
    if recipe.get('recipeInstructions'):
        steps_text = '\n'.join([f"{i+1}. {step}" for i, step in enumerate(recipe['recipeInstructions'])])
        content_parts.append(f"æ­¥éª¤ï¼š\n{steps_text}")
    
    # å…³é”®è¯
    if recipe.get('keywords'):
        keywords = ', '.join(recipe['keywords'])
        content_parts.append(f"å…³é”®è¯ï¼š{keywords}")
    
    content = '\n\n'.join(content_parts)
    
    # æ„å»ºå…ƒæ•°æ®
    metadata = {
        "name": recipe.get('name', ''),
        "dish": recipe.get('dish', 'Unknown'),
        "author": recipe.get('author', ''),
    }
    
    return Document(page_content=content, metadata=metadata)


def import_batch(documents: List[Document], batch_num: int, total_batches: int) -> tuple:
    """
    å¯¼å…¥ä¸€æ‰¹æ–‡æ¡£ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        batch_num: æ‰¹æ¬¡ç¼–å·
        total_batches: æ€»æ‰¹æ¬¡æ•°
    
    Returns:
        tuple: (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡, é”™è¯¯ä¿¡æ¯)
    """
    try:
        vectorstore = get_vectorstore()
        # ä½¿ç”¨é”ç¡®ä¿å†™å…¥æ“ä½œçš„çº¿ç¨‹å®‰å…¨
        # æ³¨æ„ï¼šLangChain çš„ add_documents å†…éƒ¨ä¼šæ‰¹é‡è°ƒç”¨åµŒå…¥ API
        with _write_lock:
            doc_ids = vectorstore.add_documents(documents)
        return (len(doc_ids), 0, None)
    except ImportError as e:
        # å¤„ç†ç¼ºå°‘ä¾èµ–çš„æƒ…å†µ
        error_msg = str(e)
        if 'dashscope' in error_msg.lower():
            return (0, len(documents), "ç¼ºå°‘ dashscope åŒ…ï¼Œè¯·è¿è¡Œ: pip install dashscope")
        return (0, len(documents), f"å¯¼å…¥é”™è¯¯: {error_msg}")
    except Exception as e:
        return (0, len(documents), str(e))


def import_recipes(
    json_file: str,
    collection_name: str = None,
    batch_size: int = 200,
    max_workers: int = 5
):
    """
    å¹¶å‘å¯¼å…¥èœè°±æ•°æ®åˆ°å‘é‡æ•°æ®åº“
    
    Args:
        json_file: JSON æ–‡ä»¶è·¯å¾„
        collection_name: é›†åˆåç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
        batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡
        max_workers: æœ€å¤§å¹¶å‘æ•°
    """
    start_time = time.time()
    print(f"ğŸ“– å¼€å§‹å¯¼å…¥èœè°±æ•°æ®ï¼š{json_file}")
    print(f"âš™ï¸  é…ç½®ï¼šæ‰¹æ¬¡å¤§å°={batch_size}, å¹¶å‘æ•°={max_workers}")
    
    # åŠ è½½æ•°æ®
    try:
        recipes = load_json_data(json_file)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(recipes)} æ¡èœè°±æ•°æ®")
    except Exception as e:
        print(f"âŒ åŠ è½½ JSON æ–‡ä»¶å¤±è´¥ï¼š{e}")
        return
    
    # è½¬æ¢ä¸ºæ–‡æ¡£
    print("ğŸ”„ è½¬æ¢æ–‡æ¡£æ ¼å¼...")
    documents = []
    failed_conversions = 0
    for recipe in recipes:
        try:
            doc = recipe_to_document(recipe)
            documents.append(doc)
        except Exception as e:
            failed_conversions += 1
            if failed_conversions <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                print(f"âš ï¸  è½¬æ¢èœè°±å¤±è´¥ï¼ˆè·³è¿‡ï¼‰ï¼š{e}")
            continue
    
    if failed_conversions > 5:
        print(f"âš ï¸  è¿˜æœ‰ {failed_conversions - 5} ä¸ªè½¬æ¢å¤±è´¥")
    
    if not documents:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å¯å¯¼å…¥")
        return
    
    total_docs = len(documents)
    print(f"ğŸ“ å‡†å¤‡å¯¼å…¥ {total_docs} ä¸ªæ–‡æ¡£...")
    
    # åˆ†æ‰¹å¤„ç†
    batches = []
    for i in range(0, total_docs, batch_size):
        batch = documents[i:i + batch_size]
        batches.append(batch)
    
    total_batches = len(batches)
    print(f"ğŸ“¦ åˆ†ä¸º {total_batches} æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹ {batch_size} æ¡ï¼‰")
    
    # å¹¶å‘å¯¼å…¥
    success_count = 0
    fail_count = 0
    completed_batches = 0
    
    print("\nğŸš€ å¼€å§‹å¹¶å‘å¯¼å…¥...")
    print("=" * 60)
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_batch = {
            executor.submit(import_batch, batch, i + 1, total_batches): i + 1
            for i, batch in enumerate(batches)
        }
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_batch):
            batch_num = future_to_batch[future]
            try:
                success, failed, error = future.result()
                success_count += success
                fail_count += failed
                completed_batches += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                progress = (completed_batches / total_batches) * 100
                print(f"[{completed_batches}/{total_batches}] æ‰¹æ¬¡ {batch_num} å®Œæˆ: "
                      f"âœ… {success} æ¡æˆåŠŸ, âŒ {failed} æ¡å¤±è´¥ "
                      f"(è¿›åº¦: {progress:.1f}%)")
                
                if error:
                    print(f"   é”™è¯¯: {error}")
            except Exception as e:
                fail_count += len(batches[batch_num - 1])
                completed_batches += 1
                print(f"[{completed_batches}/{total_batches}] æ‰¹æ¬¡ {batch_num} å¼‚å¸¸: {e}")
    
    # ç»Ÿè®¡ç»“æœ
    elapsed_time = time.time() - start_time
    print("=" * 60)
    print(f"\nâœ… å¯¼å…¥å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
    print(f"   - æ€»æ–‡æ¡£æ•°: {total_docs}")
    print(f"   - æˆåŠŸå¯¼å…¥: {success_count}")
    print(f"   - å¤±è´¥æ•°é‡: {fail_count}")
    print(f"   - è€—æ—¶: {elapsed_time:.2f} ç§’ ({elapsed_time/60:.2f} åˆ†é’Ÿ)")
    if success_count > 0:
        print(f"   - å¹³å‡é€Ÿåº¦: {success_count/elapsed_time:.2f} æ¡/ç§’")
    print(f"ğŸ“Š æ•°æ®åº“è·¯å¾„ï¼š{settings.CHROMA_DB_PATH}")
    print(f"ğŸ“š é›†åˆåç§°ï¼š{collection_name or settings.RAG_COLLECTION_NAME}")


def main():
    parser = argparse.ArgumentParser(description="å¯¼å…¥èœè°±æ•°æ®åˆ° RAG å‘é‡æ•°æ®åº“ï¼ˆå¹¶å‘ä¼˜åŒ–ç‰ˆï¼‰")
    parser.add_argument(
        "--input",
        "-i",
        required=True,
        help="è¾“å…¥çš„ JSON æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--collection",
        "-c",
        default=None,
        help="å‘é‡æ•°æ®åº“é›†åˆåç§°ï¼ˆå¯é€‰ï¼‰"
    )
    parser.add_argument(
        "--batch-size",
        "-b",
        type=int,
        default=200,
        help="æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤: 200ï¼‰"
    )
    parser.add_argument(
        "--workers",
        "-w",
        type=int,
        default=5,
        help="æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤: 5ï¼Œå»ºè®®ä¸è¶…è¿‡ 10 ä»¥é¿å… API é™æµï¼‰"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.input).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{args.input}")
        return
    
    # æ£€æŸ¥é…ç½®
    if not settings.DASHSCOPE_API_KEY:
        print("âš ï¸  è­¦å‘Šï¼šDASHSCOPE_API_KEY æœªé…ç½®ï¼Œå°†æ— æ³•ç”Ÿæˆå‘é‡")
        print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DASHSCOPE_API_KEY")
        return
    
    # å‚æ•°éªŒè¯
    if args.batch_size < 1:
        print("âŒ æ‰¹æ¬¡å¤§å°å¿…é¡»å¤§äº 0")
        return
    
    if args.workers < 1:
        print("âŒ å¹¶å‘æ•°å¿…é¡»å¤§äº 0")
        return
    
    if args.workers > 20:
        print("âš ï¸  è­¦å‘Šï¼šå¹¶å‘æ•°è¿‡å¤§å¯èƒ½å¯¼è‡´ API é™æµï¼Œå»ºè®®ä¸è¶…è¿‡ 10")
    
    # æ‰§è¡Œå¯¼å…¥
    import_recipes(
        args.input,
        args.collection,
        batch_size=args.batch_size,
        max_workers=args.workers
    )


if __name__ == "__main__":
    main()
